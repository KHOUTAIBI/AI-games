{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins> Deep Q Learning <ins>:\n",
    "-This Notebook serves as a step by step illustration of the **Deep Q Learning** code used to train agents in a certain environment. This code belows extends the **Deep Q Learning** method by adding another target network, we shall code this method **Double Deep Q Learning** or **Double DQL** for short.\n",
    "\n",
    "-This algorithm uses classes and the Bellman Equation that will be introduced bellow \n",
    "\n",
    "**Let us start importing the necessary libraries for the following code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optimum\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack, record_video,RecordVideo\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pk\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imported many libraries, but to be short, we mainly imported **Pytorch** and **gymnasium** libraries and sub-libraries. **gymnasium.wrappers** is a sub gymnasium library used to add the different preprocessing utils that reshape, grayscale, framestack and occasionnaly record the developpement of our environment.\n",
    "\n",
    "Now we can start constructing our classes wich will contain many useful functions following the DQL algorithm.\n",
    "\n",
    "## <ins>Neural Network Class<ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, input_dims,n_actions):\n",
    "        super(DeepQNetwork,self).__init__()\n",
    "\n",
    "        self.input_dims = input_dims\n",
    "        self.conv1 = nn.Conv2d(4,16,8,stride=4)\n",
    "        self.conv2 = nn.Conv2d(16,32,4,stride=2)     \n",
    "        self.conv3 = nn.Conv2d(32,32,3,stride=1)\n",
    "        self.fc1 = nn.Linear(32*7*7, 256)\n",
    "        self.fc2 = nn.Linear(256, n_actions)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = T.as_tensor(x)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x).flatten(start_dim=1))\n",
    "        x = self.relu(self.fc1(x)) \n",
    "        return self.relu(self.fc2(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<ins>NOTE <ins>** : The neural network parameters were used are the same ones found in the 2015 DQL paper, we have chosen these values since according to the paper, we can get promissing resuts with minimal processing time.\n",
    "\n",
    "**<ins> How does the depth of a neural network change? <ins>**\n",
    "The depth of a neural network may help to greatly approximate the probabilities at the end of the network. However, the computation time increases significantly. In the beginning we had inputs of **64**, but we changed it to **32** for the sake of computation and because the environment aren't that complex\n",
    "\n",
    "## <ins>Memory/Experience Replay<ins>: \n",
    "Now let us define our Memory class, which really highlights the **Deep** in **Deep Q Learning**. The agent will learn using previous iterations, a sort of buffer where we compare the current observation with randomly sampled memories to compute the gradient and finally converge the Q value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self,input_dims,max_mem_size = 20000):\n",
    "        self.mem_size = max_mem_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        #intialise the memory\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype = np.float32)\n",
    "        self.new_state_mem = np.zeros((self.mem_size,*input_dims),dtype = np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size,dtype= np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype= np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size,dtype = bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_neu, done):\n",
    "        #storing the memories\n",
    "        \n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_mem[index] = state_neu\n",
    "        self.reward_memory [index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <ins>Agent<ins>:\n",
    "Let us now construct the agent, **the fundemental** and **core** part of the **DQL Algorithm**. We instansiate the fundemental parts of a Neural Network such as : **The optimizer : Adam in our case**  , **the loss function : Huber here**, **the memory sections**, and another **gamma** that shall be used later .\n",
    "\n",
    "We now enter the **learning section**, in here we use **the Bellman Equation** in order to converge the **Q_values** to a certain Matrix optimal for the environment. \n",
    "## <ins>Epsilon Greey Approach<ins>:\n",
    "We follow the $\\epsilon$-greedy algorithm used in DQL, this approach is used so that the agent explores the maximum of possibilities in a certain environment. This helps avoid situations where an agent finds a local loss minimum and, well, stays there. We had this happen where the agent found it self getting the maximum rewards by just holding a certain position in breakout and pong. \n",
    "Furthermore, this $\\epsilon$ decays as time goes by, meaning that the agent becomes more and more independant and has to compute the best action in its environment following his Q values.\n",
    " \n",
    "## <ins>Learning<ins> \n",
    "\n",
    "We start off by filling up the buffer so that we get at least the minimum of random environment, actions, rewards, terminal states and new observation states.\n",
    "This code bellow shows that for mem_cntr \\< min_mem == 4000, the agent **will not learn**. \n",
    "After the buffer is filled, we choose random numbers and random **(states,new states,rewards, terminals states , actions states)** that will be used to compute the gradient in our current situation in order for **Q** to converge.\\\n",
    "\n",
    "We then define a supplementary Q matrix that we call **q_target**, this will serve as our secondary Q matrix.\n",
    "\n",
    "## <ins>Q Target<ins>\n",
    "### But why add another Q matrix ? Doesn't it complicate the code ? Why not evaluate the new state and the current state in just a single Q Matrix ?\n",
    "\n",
    "Well there a many reasons but essentially:\\\n",
    "    The problem with Q-Learning is that the same samples are being used to decide which action is the best (highest expected reward), and the same samples are also being used to estimate that action-value as shown in the Bellman Equation : \n",
    "\n",
    "$$Q(s, a) = R(s, a) + \\gamma \\cdot \\max_{a'} Q(s', a')$$\n",
    "\n",
    "if an action’s value was overestimated, it will be chosen as the best action, and it’s overestimated value is used as the target\n",
    "\n",
    "The Target Q value helps to not oerestimate an action following a certain states. The weights of the Q_target matrix are then updated to correspond with the weights of the Q Matrix as not to diverge from it.\n",
    "\n",
    "**NOTE** : We follow the 2015 paper and compute the following equation for the Q matrix. We only update the Q value if the state is not terminated, this helps in the agent's convergence.\n",
    "\n",
    "$$Q(s, a) = R(s, a) + \\gamma \\cdot\\ (1-terminated) \\cdot \\max_{a'} Q(s', a')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agents():\n",
    "    #initialising agent\n",
    "    def __init__(self, gamma, lr, batch_size, n_actions, input_dims, epsilon, eps_end=0.1, eps_dec = 0.9999, max_mem_size = 20000, min_mem = 4000):\n",
    "        \n",
    "        self.device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "        self.gamma= gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.min_mem = min_mem\n",
    "        self.action_space  = [i for i in range(n_actions)]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.memory = Memory(input_dims,max_mem_size)\n",
    "\n",
    "        self.Q_eval = DeepQNetwork(input_dims, n_actions).to(self.device)\n",
    "        self.optimizer = optimum.Adam(self.Q_eval.parameters(),lr=self.lr,eps=1e-7)\n",
    "        self.loss_fn = nn.HuberLoss()\n",
    "\n",
    "    #choosing a random action depending of the value of epsilon\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = np.array([observation],dtype=np.float32)\n",
    "            state = T.tensor(state).to(self.device)\n",
    "            actions = self.Q_eval(state)\n",
    "            return T.argmax(actions).item()\n",
    "        else:\n",
    "            return np.random.choice(self.action_space)\n",
    "    \n",
    "    def learn(self, target_res , terminated):\n",
    "        if self.memory.mem_cntr < self.min_mem:\n",
    "            return\n",
    "\n",
    "        ##Randomly selecting a batch index\n",
    "\n",
    "        max_mem = min(self.memory.mem_cntr, self.memory.mem_size)\n",
    "        batch = np.random.choice(max_mem,self.batch_size, replace= False)\n",
    "        batch_index = np.arange(self.batch_size,dtype = np.int32)\n",
    "        \n",
    "        #randomly selecting states\n",
    "        state_batch =  T.tensor(self.memory.state_memory[batch]).to(self.device)\n",
    "        new_state_batch = T.tensor(self.memory.new_state_mem[batch]).to(self.device)\n",
    "        reward_batch = T.tensor(self.memory.reward_memory[batch]).to(self.device)\n",
    "\n",
    "        action_batch = self.memory.action_memory[batch]\n",
    "        \n",
    "        #evaluating the state\n",
    "        q_eval = self.Q_eval(state_batch)[batch_index,action_batch]\n",
    "        \n",
    "        q_next = target_res(new_state_batch)\n",
    "\n",
    "        #computing the target vale\n",
    "        q_target = reward_batch + (1 - terminated)*self.gamma * T.max(q_next, dim=1)[0]\n",
    "\n",
    "        #back propegating the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_fn(q_target, q_eval).to(self.device)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>How does the Learning rate affect the Network ?<ins>\n",
    "The learning rate used in the Adam optimizer plays a major role for Q Learning. Big values may cause the Network to be extremely unstable. While small learning rates may lead to no learning at all ! \n",
    "We had this problem where with big values the network was unstable, so we kept decreasing it (to 0.00025) where he had better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <ins>Training<ins>:\n",
    "Let us now Start the agents' Training.  We import the **Pong Environment** and we *wrap* this environment by some preprocessing preprocesses. **TheAtari preprocessing** basically **rescales the imgae to 84x 84 , grayscales the image, and skips four frames ie only the fourth frame matters so we explore more actions**.\\\n",
    "Finally we Stack four frames in order to process the maximum number of environment, we end up finally with a **(4,1,84,84) Tensor**. \n",
    "\n",
    "**NOTE :** You may have noticed that we have a *frame_skip = 1*, this is because this operation is already done by the ALE library, we would not want to skip 8 frames !\\\n",
    "Removing *frame_skip = 1* will result in an error !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\", render_mode=None) \n",
    "#frame_skip =1 because the ALE library already does a frame_skip = 4\n",
    "env = AtariPreprocessing(env, frame_skip=1) \n",
    "#Stack four frames to speed up the process\n",
    "env = FrameStack(env, 4) \n",
    "n_episodes = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make two differents Q matrixes and we initialise an agent with the following values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = Agents(gamma= 0.99, lr = 0.00025, batch_size=64, n_actions=env.action_space.n, input_dims=(4,84,84), epsilon=1, eps_dec=0.99999)\n",
    "target_res = DeepQNetwork(env.observation_space.shape, env.action_space.n).to(T.device('cuda' if T.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure we are using a GPU and not a CPU, these computations take a **LONG** time so make sure to have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if T.cuda.is_available():\n",
    "    print(\"Running on GPU :\", T.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the 2015 Paper, we clip the rewards between -1 and 1 since,according to the authors, it helps with the agent's convergence and helps to avoid divergence. Furthermore, the rewards of some environment are difficult to control, making the clipping manageable.\n",
    "\n",
    "We now can effectively start training. This takes a **LONG LONG** time , so sit back and watch the agent accumulate rewards over time ! We update the Q Target network every 10 interations as stated above.\\\n",
    "We also save the Network's weights every 30 iterations or when interrupting so as to continue training later on.\\\n",
    "We finally plot the rewards after finishing the training or when interrupting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rewards_per_episode = []\n",
    "try:\n",
    "    for i in range(n_episodes+1):\n",
    "        rewards = 0\n",
    "        time = 0\n",
    "        terminated = False\n",
    "        tronc = False\n",
    "        \n",
    "        ## We initialise the environment using env.reset()\n",
    "        \n",
    "        observation = env.reset()[0]\n",
    "        \n",
    "        ## We execute a while loop, whre for a certain observation state, we get an action, then we compute:\n",
    "        ## observation_neu, reward, terminated, _, info = env.step(action) , this gives us the useful parameters defined above\n",
    "        ## we Store these values in our memory\n",
    "        ## we execute the learning function\n",
    "        ## we finally update the observation with the new observation\n",
    "        \n",
    "        while not terminated:\n",
    "            action = agent.choose_action(observation)\n",
    "            reward = 0\n",
    "            \n",
    "            #new observation state\n",
    "            observation_neu, reward, terminated, _, info = env.step(action)\n",
    "            \n",
    "            #clipping the rewards\n",
    "            reward = np.clip( reward, a_min=-1, a_max=1 )\n",
    "            \n",
    "            #storing the resuls and learning\n",
    "            agent.memory.store_transition(observation,action,reward,observation_neu,terminated)\n",
    "            agent.learn(target_res,terminated)\n",
    "            \n",
    "            #updating the oservation\n",
    "            observation  = observation_neu\n",
    "            rewards += reward\n",
    "\n",
    "            ## We decay the epsilon following geometric decay\n",
    "            agent.epsilon = max(agent.epsilon*agent.eps_dec, agent.eps_min)\n",
    "\n",
    "        rewards_per_episode.append(rewards)\n",
    "        mean_rewards = np.mean(rewards_per_episode[len(rewards_per_episode)-100:])\n",
    "\n",
    "        # Update target weights\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            target_res.load_state_dict(agent.Q_eval.state_dict())\n",
    "        \n",
    "        if i%30==0:\n",
    "            text = \"save_w_checkpoint_\"+str(0)+\".pth\"\n",
    "            T.save(agent.Q_eval.state_dict(),\"./\"+text)\n",
    "            print(f'Episode: {i}, Rewards: {rewards},  Epsilon: {agent.epsilon:0.2f}, Mean Rewards: {mean_rewards:0.1f}')\n",
    "    \n",
    "    #Plotting the rewards and saving the results for future iterations\n",
    "    env.close()\n",
    "    T.save(agent.Q_eval.state_dict(), \"save_w_break.pth\")\n",
    "    plt.plot(rewards_per_episode)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"rewards\")\n",
    "    plt.savefig(\"./rewardplot.png\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    #Plotting the rewards and saving the results for future iterations\n",
    "    env.close()\n",
    "    T.save(agent.Q_eval.state_dict(), \"save_w_interrupt.pth\")\n",
    "    plt.plot(rewards_per_episode)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"rewards\")\n",
    "    plt.savefig(\"./rewardplot.png\")\n",
    "    sys.exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulation**, you have finished training, Now let us see the agents results. if everything goes according to plan you will get this figure approximately:\n",
    "![reward plot](./rewardplot.png)\n",
    "## <ins>Rendering<ins>:\n",
    "\n",
    "Simply change the default **render mode** from **None** to **human** to see your agent playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"human\") \n",
    "env = AtariPreprocessing(env,frame_skip=1) \n",
    "env = FrameStack(env, 4) \n",
    "recorder = False\n",
    "\n",
    "if recorder:\n",
    "    env = RecordVideo(env, video_folder=\"./videos\", name_prefix=\"Breakout_best\",\n",
    "                      episode_trigger=lambda x: x %100 == 0)\n",
    "n_episodes = 100\n",
    "\n",
    "agent = Agents(gamma= 0.99, lr = 0.00025, batch_size=32, n_actions=env.action_space.n, input_dims=env.observation_space.shape, epsilon=0.001)\n",
    "\n",
    "rewards_per_episode = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    rewards = 0\n",
    "    terminated = False\n",
    "    observation = env.reset()[0]\n",
    "\n",
    "    ## We execute a while loop, whre for a certain observation state, we get an action, then we compute:\n",
    "    ## observation_neu, reward, terminated, _, info = env.step(action) , this gives us the useful parameters defined above\n",
    "    ## we Store these values in our memory\n",
    "    ## we execute the learning function\n",
    "    ## we finally update the observation with the new observation\n",
    "    \n",
    "    while not terminated:\n",
    "        action = agent.choose_action(observation)\n",
    "        new_observation, reward, terminated, _, info =  env.step(action)\n",
    "        rewards += reward\n",
    "        observation = new_observation\n",
    "        \n",
    "\n",
    "    rewards_per_episode.append(rewards)\n",
    "    mean_rewards = np.mean(rewards_per_episode[-100:])\n",
    "    \n",
    "    print(f'Episode: {i}, Rewards: {rewards},  Epsilon: {agent.epsilon:0.2f}, Mean Rewards: {mean_rewards:0.1f}')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./eval-episode-0.mp4\" controls  width=\"450\"  height=\"450\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video('./eval-episode-0.mp4', width=450, height=450)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
